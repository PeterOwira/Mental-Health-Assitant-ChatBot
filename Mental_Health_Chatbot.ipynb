{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Mental Health Assistant Chatbot**\n",
        "\n",
        "This project aims to create an Emotionally Intelligent Chatbot for Mental Health support. It combines two key components: an `Emotion Classifier Model` created by [SamLowe](https://huggingface.co/SamLowe/roberta-base-go_emotions) and the `Llama language model` created by [NousResearch](https://huggingface.co/NousResearch/Llama-2-7b-chat-hf). The emotion classifier analyzes user input to determine their emotional state, providing detailed scores for emotions like sadness, joy, and anger. This emotional context is then integrated into prompts for `Llama 2`, enabling it to generate responses that are both contextually relevant and emotionally appropriate.\n",
        "The chatbot is designed to detect when users are feeling down and provide supportive, empathetic responses. It acknowledges the user's emotional state, addresses their concerns, and offers encouragement. This system aims to provide a supportive presence for users, especially during times of emotional distress.\n",
        "\n",
        "Key features include:\n",
        "1.   Real-time emotion analysis of user input\n",
        "2.   Context-aware response generation using Llama 2\n",
        "3.   Tailored support based on detected emotional states\n",
        "4.   Conversational communication style\n",
        "\n",
        "While not a replacement for professional mental health care, this chatbot serves as a supplementary tool for emotional support and a potential first point of contact for those seeking help. The project shows the importance of responsible AI implementation in mental health applications, including appropriate disclaimers and safeguards."
      ],
      "metadata": {
        "id": "S_snnc8iO68G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Installing Dependencies**\n",
        "In this section, we install the necessary libraries for working with the Hugging Face Transformers library, Llama 2, and PyTorch.\n",
        "We will install additional utilities such as `huggingface_hub` for managing models and `sentencepiece` for tokenization."
      ],
      "metadata": {
        "id": "Re7XT3sxoSnd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqKJrtz3rIk2"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate protobuf sentencepiece torch git+https://github.com/huggingface/transformers huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Importing Libraries**\n",
        "Here, we import the necessary Python libraries for handling language models, tokenization, and model authentication.\n",
        "- `transformers`: Provides access to pre-trained language models.\n",
        "- `AutoModelForCausalLM`, `AutoTokenizer`: Used for loading Llama 2.\n",
        "- `torch`: PyTorch framework for deep learning.\n",
        "- `pipeline`: Utility for setting up pipelines like the emotion classifier."
      ],
      "metadata": {
        "id": "jkFk1Sb9ofK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "8KT9itlPtV86"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Hugging Face Authentication**\n",
        "We authenticate with Hugging Face using a pre-generated token stored in Colab.\n",
        "This allows us to access and load models hosted on Hugging Face."
      ],
      "metadata": {
        "id": "xnNypd9aoyKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face Authentication\n",
        "login(token=userdata.get('hugging_token_secret'))"
      ],
      "metadata": {
        "id": "PSgJD9-QteIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Initializing the Models**\n",
        "We start loading the models using pipelines from Hugging Face.\n"
      ],
      "metadata": {
        "id": "LbBNJZ8yo2vU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the our emotional classifier model by passing the parameters\n",
        "\n",
        "emotion_classifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)"
      ],
      "metadata": {
        "id": "RvnOQWL54hJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Llama 2 model and tokenizer\n",
        "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.use_default_system_prompt = False\n",
        "\n",
        "# Initialize the pipeline using Hugging Face pipeline\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",  # LLM task\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    max_length=1024,  # Adjust max_length as needed\n",
        ")"
      ],
      "metadata": {
        "id": "-l9BeO-ZuQ3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing our loaded models"
      ],
      "metadata": {
        "id": "S3XYWbgLrv8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = emotion_classifier(\"I feel disgusted.\", )\n",
        "print(prediction[0])"
      ],
      "metadata": {
        "id": "AIIlRKkA-_ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction2 = llama_pipeline(\"What is Mental Health?\", max_length=1024, do_sample=True)\n",
        "print(prediction2[0]['generated_text'])"
      ],
      "metadata": {
        "id": "8FLmui5jqeaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Creation of the User input processing function**\n",
        "We start by creating the main function `process_user_message` for analyzng and processing the text provided by the user together with its utility functions.\n",
        "\n",
        "Below is a detailed explanation of the utility functions:\n",
        "\n",
        "\n",
        " 1. `get_primary_emotion function`:  This function determines the most prominent emotion based on the highest score.\n",
        "\n",
        "2.  `create_emotion_aware_prompt function`:\n",
        "This function crafts a prompt for Llama 2 that includes:\n",
        "    *  *The user's original input.*\n",
        "    *  *A detailed breakdown of their emotional state.*\n",
        "    *  *Guidelines for generating an appropriate response.*\n",
        "\n",
        "3.  `generate_emotion_context function:`\n",
        "This function creates a detailed description of the user's emotional state, including the primary emotion and a breakdown of all emotion scores.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mi5LKXw2r-ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_user_message(user_input):\n",
        "    # Step 1: Process user input\n",
        "    emotion_scores = emotion_classifier(user_input)[0]\n",
        "\n",
        "    # Step 2: Analyze emotions\n",
        "    primary_emotion = get_primary_emotion(emotion_scores)\n",
        "\n",
        "    # Step 3: Generate response using Llama\n",
        "    prompt = create_emotion_aware_prompt(user_input, emotion_scores, primary_emotion)\n",
        "    response = llama_pipeline(prompt, max_length=1024, do_sample=True)[0]['generated_text']\n",
        "\n",
        "    # Ensure the response doesn't redundantly include the question or incorrectly repeat \"Answer\"\n",
        "    response = response.replace(f\"{prompt}\", \"\").strip()\n",
        "\n",
        "    # Step 4: Deliver response\n",
        "    return response\n",
        "\n",
        "\n",
        "## Utility functions for emotional intergrations with LLM\n",
        "def get_primary_emotion(emotion_scores):\n",
        "    return max(emotion_scores, key=lambda x: x['score'])['label']\n",
        "\n",
        "def create_emotion_aware_prompt(user_input, emotion_scores, primary_emotion):\n",
        "    emotion_context = generate_emotion_context(emotion_scores, primary_emotion)\n",
        "\n",
        "    return f\"\"\"\n",
        "    The user has expressed the following: \"{user_input}\"\n",
        "\n",
        "    Emotional analysis:\n",
        "    {emotion_context}\n",
        "\n",
        "    Please provide a response that:\n",
        "    1. Acknowledges and validates the user's emotional state\n",
        "    2. Addresses the content of their message\n",
        "    3. Offers support or encouragement appropriate to their emotional state\n",
        "    4. Maintains a conversational and empathetic tone\n",
        "\n",
        "    Generate a response:\n",
        "    \"\"\"\n",
        "\n",
        "def generate_emotion_context(emotion_scores, primary_emotion):\n",
        "    context = f\"The user's primary emotion appears to be {primary_emotion}.\\n\"\n",
        "    context += \"Emotional state breakdown:\\n\"\n",
        "\n",
        "    for emotion in emotion_scores:\n",
        "        context += f\"- {emotion['label']}: {emotion['score']:.4f}\\n\"\n",
        "\n",
        "    return context"
      ],
      "metadata": {
        "id": "6Z6waOEnDg8V"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Testing our Text Processing function**"
      ],
      "metadata": {
        "id": "xlzXhvI-uvmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example usage\n",
        "user_input = \"I am fed up with the constant disregard for my needs and feelings, and I demand respect and acknowledgment from those who consistently disregard my boundaries.\"\n",
        "response = process_user_message(user_input)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "53NHeDkWVtzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Creating User Interface Chatbot using Gradio**\n",
        "\n",
        "Step 1:   We start by installing gradio dependancies.\n",
        "\n",
        "Step 2:  We create a function for gradio to handle user inputs whereby the function processes the user input and returns the response\n",
        "\n",
        "Step 3:   We create the Gradio interface and launch it to generate the ChatBot."
      ],
      "metadata": {
        "id": "DIisUpaevJxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gradio"
      ],
      "metadata": {
        "id": "EBzhFHmbwlh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def gradio_chat_interface(user_input):\n",
        "\n",
        "  response = process_user_message(user_input)\n",
        "  return response\n",
        ""
      ],
      "metadata": {
        "id": "EkwNnb4ywtXn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Gradio Interface\n",
        "interface = gr.Interface(\n",
        "    fn=gradio_chat_interface,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"Llama 2 Mental Health Chat Assistant\",\n",
        "    description=\"Let us talk Today\",\n",
        ")"
      ],
      "metadata": {
        "id": "kx94du1iw1vV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the Gradio Interface\n",
        "interface.launch()"
      ],
      "metadata": {
        "id": "97lYtYhsw7Vq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}